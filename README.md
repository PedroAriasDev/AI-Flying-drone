# ğŸš Drone Gesture Control System

Sistema de control de dron mediante gestos de manos utilizando visiÃ³n por computadora y redes neuronales profundas.

**Proyecto Final - Inteligencia Artificial**

---

## ğŸ“‹ DescripciÃ³n

Este proyecto implementa un sistema completo para controlar un dron virtual (simulador 3D) usando gestos de manos capturados por webcam. El sistema utiliza:

- **MediaPipe Hands**: DetecciÃ³n de manos, segmentaciÃ³n automÃ¡tica y extracciÃ³n de landmarks (21 puntos)
- **Red Clasificadora (CNN)**: Clasifica el gesto entre 11 clases. Se comparan 4 arquitecturas: ResNet18, ResNet34, MobileNetV3-Large y MobileNetV3-Small
- **Red Temporal (GRU)**: Analiza secuencias de 30 frames para suavizado y detecciÃ³n de intensidad del gesto

---

## ğŸ® Gestos Soportados

| Gesto | Comando | DescripciÃ³n |
|-------|---------|-------------|
| âœ‹ Palma adelante | PITCH_FORWARD | Mover dron hacia adelante |
| ğŸ–ï¸ Palma vertical | PITCH_BACKWARD | Mover dron hacia atrÃ¡s |
| âœŒï¸ V-dedos derecha | ROLL_RIGHT | Mover lateralmente a la derecha |
| âœŒï¸ V-dedos izquierda | ROLL_LEFT | Mover lateralmente a la izquierda |
| ğŸ‘ Pulgar arriba | THROTTLE_UP | Subir altitud |
| ğŸ‘ Pulgar abajo | THROTTLE_DOWN | Bajar altitud |
| ğŸ¤™ Shaka derecha | YAW_RIGHT | Rotar en sentido horario |
| ğŸ¤™ Shaka izquierda | YAW_LEFT | Rotar en sentido antihorario |
| âœŠ PuÃ±o cerrado | HOVER | Mantener posiciÃ³n |
| ğŸ–– Vulcano | EMERGENCY_STOP | Parada de emergencia |

---

## ğŸ“ Estructura del Proyecto

```
AI-Flying-drone/
â”œâ”€â”€ config.py                      # ConfiguraciÃ³n global
â”œâ”€â”€ main.py                        # Script principal
â”œâ”€â”€ dataset_recorder.py            # Grabador de dataset
â”œâ”€â”€ inference.py                   # Sistema de inferencia en tiempo real
â”œâ”€â”€ drone_simulator.py             # Simulador 3D mejorado con paisaje
â”œâ”€â”€ datasets.py                    # Clases de Dataset PyTorch
â”œâ”€â”€ training_utils.py              # Utilidades de entrenamiento
â”œâ”€â”€ train_classifier.py            # Entrenamiento de CNN individual
â”œâ”€â”€ train_classifier_compare.py   # ğŸ†• Entrenamiento comparativo de 4 modelos
â”œâ”€â”€ train_temporal.py              # Entrenamiento de GRU (30 frames)
â”œâ”€â”€ visualize_architectures.py    # ğŸ†• VisualizaciÃ³n de arquitecturas
â”œâ”€â”€ CAMBIOS_Y_MEJORAS.md           # ğŸ†• DocumentaciÃ³n de cambios
â”œâ”€â”€ GUIA_RAPIDA.md                 # ğŸ†• GuÃ­a de uso rÃ¡pida
â”œâ”€â”€ requirements.txt               # Dependencias
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ classifier.py              # 4 modelos CNN comparables
â”‚   â”œâ”€â”€ temporal.py                # Modelo GRU temporal (30 frames)
â”‚   â””â”€â”€ segmentation.py            # (NO USADO - solo legacy)
â”œâ”€â”€ data/
â”‚   â””â”€â”€ dataset/                   # Dataset de gestos
â”œâ”€â”€ checkpoints/                   # Modelos entrenados
â”œâ”€â”€ results/                       # Resultados y grÃ¡ficos comparativos
â””â”€â”€ logs/                          # Logs de entrenamiento
```

---

## ğŸš€ InstalaciÃ³n

### 1. Clonar el repositorio
```bash
git clone <tu-repositorio>
cd drone_gesture_control
```

### 2. Crear entorno virtual
```bash
python -m venv venv
source venv/bin/activate  # Linux/Mac
# o
venv\Scripts\activate     # Windows
```

### 3. Instalar dependencias
```bash
# Para GPU NVIDIA (CUDA 11.8)
pip install torch torchvision --index-url https://download.pytorch.org/whl/cu118

# Resto de dependencias
pip install -r requirements.txt
```

---

## ğŸ“Š Flujo de Trabajo

### Paso 1: Grabar Dataset

```bash
python main.py --mode record
```

**Controles del grabador:**
- `0-9`: Seleccionar clase de gesto
- `ESPACIO`: Iniciar/Pausar grabaciÃ³n
- `S`: Guardar estadÃ­sticas
- `T`: Guardar secuencia temporal
- `Q`: Salir

**Recomendaciones de grabaciÃ³n:**
- 8-10 minutos por gesto
- Variar iluminaciÃ³n (natural, artificial)
- Variar distancia a la cÃ¡mara (50cm, 1m, 1.5m)
- Variar Ã¡ngulos de la mano
- Total: ~1.5-2 horas de grabaciÃ³n

### Paso 2: Entrenar Modelos (PC Local - Optimizado)

```bash
# 1. Entrenar y comparar 4 clasificadores CNN automÃ¡ticamente (100 Ã©pocas)
python train_classifier_compare.py

# 2. Entrenar red temporal GRU (30 frames, 100 Ã©pocas)
python train_temporal.py --epochs 100 --batch_size 32

# 3. (Opcional) Visualizar arquitecturas de las redes
python visualize_architectures.py
```

**CaracterÃ­sticas del nuevo sistema:**
- âœ… Entrenamiento optimizado para GPU local (no Colab)
- âœ… 100 Ã©pocas con batch size 256
- âœ… ComparaciÃ³n automÃ¡tica de 4 modelos
- âœ… SelecciÃ³n del mejor modelo basada en mÃ©tricas
- âœ… GrÃ¡ficos y anÃ¡lisis generados automÃ¡ticamente

**Tiempo estimado** (GPU RTX 3060/3070):
- Clasificadores (4 modelos): ~4-5 horas
- Red temporal: ~1-2 horas
- **Total: ~6-7 horas**

### Paso 3: Ejecutar Sistema

```bash
# Solo demo de inferencia
python main.py --mode demo

# Solo simulador (control por teclado)
python main.py --mode simulator

# Sistema integrado completo
python main.py --mode integrated
```

---

## ğŸ¯ Modos de EjecuciÃ³n

### Demo Mode
```bash
python main.py --mode demo
```
Muestra la detecciÃ³n de gestos en tiempo real con la webcam. Ãštil para probar el sistema de inferencia.

### Simulator Mode
```bash
python main.py --mode simulator
```
Ejecuta el simulador 3D del dron con control por teclado:
- `W/S`: Pitch (adelante/atrÃ¡s)
- `A/D`: Roll (izquierda/derecha)
- `Q/E`: Yaw (rotaciÃ³n)
- `ESPACIO/SHIFT`: Throttle (subir/bajar)
- `H`: Hover
- `X`: Emergencia
- `R`: Reset
- `ESC`: Salir

### Integrated Mode
```bash
python main.py --mode integrated
```
Sistema completo: la webcam captura gestos que controlan el dron en el simulador 3D.

---

## ğŸ“ˆ Arquitectura de Redes

### Red Clasificadora (CNN) - 4 Modelos Comparados

Se entrenan y comparan automÃ¡ticamente 4 arquitecturas:

1. **ResNet18**
   - ParÃ¡metros: ~11M
   - RÃ¡pido y eficiente
   - Baseline sÃ³lido

2. **ResNet34**
   - ParÃ¡metros: ~21M
   - MÃ¡s profundo, mejor capacidad
   - Mayor accuracy potencial

3. **MobileNetV3-Large**
   - ParÃ¡metros: ~5.5M
   - Optimizado para mÃ³viles
   - Buen balance velocidad/accuracy

4. **MobileNetV3-Small**
   - ParÃ¡metros: ~2.5M
   - Muy ligero y rÃ¡pido
   - Ideal para inferencia en tiempo real

**ConfiguraciÃ³n comÃºn:**
- Entrada: 224x224x3
- Salida: 11 clases de gestos
- Transfer Learning: Pre-entrenado en ImageNet
- Fine-tuning: Todas las capas

### Red Temporal (GRU Unidireccional)

- **Secuencia**: 30 frames (1 segundo @ 30fps)
- **Arquitectura**: Unidireccional (baja latencia)
- **Input**: CNN features (512D) + MediaPipe landmarks (63D)
- **Hidden Size**: 256
- **Layers**: 2
- **Outputs**:
  - ClasificaciÃ³n de gesto (11 clases)
  - Intensidad del movimiento (0-1) - detecta gestos bruscos vs suaves
- **Attention**: Mecanismo de atenciÃ³n sobre secuencia temporal

---

## ğŸ“Š MÃ©tricas Objetivo

| Modelo | MÃ©trica | Objetivo Actualizado |
|--------|---------|---------------------|
| Clasificador CNN | Test Accuracy | >97% |
| Clasificador CNN | Overfitting Score | <0.05 |
| Red Temporal GRU | Test Accuracy | >93% |
| Sistema completo | Latencia | <100ms |
| Sistema completo | FPS | â‰¥20 |

**Nuevas mÃ©tricas implementadas:**
- **Overfitting Score**: Diferencia entre train y val accuracy (menor es mejor)
- **Performance Score**: Combina val_acc (40%) + test_acc (60%) para selecciÃ³n del mejor modelo

---

## ğŸ› ï¸ ConfiguraciÃ³n

Editar `config.py` para ajustar:

```python
# ConfiguraciÃ³n de cÃ¡mara
CAMERA_CONFIG = {
    "camera_id": 0,
    "frame_width": 640,
    "frame_height": 480,
    "fps": 30,
}

# ConfiguraciÃ³n de entrenamiento
TRAINING_CONFIG = {
    "device": "cuda",
    "cls_epochs": 30,
    "cls_batch_size": 32,
    "cls_lr": 1e-4,
    # ...
}

# ConfiguraciÃ³n de inferencia
INFERENCE_CONFIG = {
    "confidence_threshold": 0.7,
    "smoothing_window": 5,
    "gesture_hold_frames": 3,
}
```

---

## ğŸ”§ SoluciÃ³n de Problemas

### Error: CUDA not available
```bash
# Verificar instalaciÃ³n de CUDA
nvidia-smi
python -c "import torch; print(torch.cuda.is_available())"

# Reinstalar PyTorch con CUDA
pip install torch torchvision --index-url https://download.pytorch.org/whl/cu118
```

### Error: No camera found
```python
# Cambiar ID de cÃ¡mara en config.py
CAMERA_CONFIG["camera_id"] = 1  # Probar 0, 1, 2...
```

### Error: OpenGL not working
```bash
# Usar modo 2D alternativo
python drone_simulator.py --2d
```

### Baja precisiÃ³n de gestos
- Aumentar cantidad de datos de entrenamiento
- Variar condiciones de grabaciÃ³n
- Ajustar `confidence_threshold` en config.py

---

## ğŸ“š Referencias

- [MediaPipe Hands](https://google.github.io/mediapipe/solutions/hands.html)
- [HaGRID Dataset](https://github.com/hukenovs/hagrid)
- [Segmentation Models PyTorch](https://github.com/qubvel/segmentation_models.pytorch)
- [PyTorch Documentation](https://pytorch.org/docs/)

---

## ğŸ‘¤ Autor

**Pedro** - Proyecto Final de Inteligencia Artificial

---

## ğŸ“„ Licencia

Este proyecto es para uso acadÃ©mico.
