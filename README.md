# ğŸš Drone Gesture Control System

Sistema de control de dron mediante gestos de manos utilizando visiÃ³n por computadora y redes neuronales profundas.

**Proyecto Final - Inteligencia Artificial**

---

## ğŸ“‹ DescripciÃ³n

Este proyecto implementa un sistema completo para controlar un dron virtual (simulador 3D) usando gestos de manos capturados por webcam. El sistema utiliza:

- **MediaPipe Hands**: DetecciÃ³n de manos y extracciÃ³n de landmarks (21 puntos)
- **Red de SegmentaciÃ³n (UNet)**: Segmenta la mano del fondo
- **Red Clasificadora (CNN)**: Clasifica el gesto entre 11 clases
- **Red Temporal (GRU)**: Analiza secuencias para suavizado y detecciÃ³n de intensidad

---

## ğŸ® Gestos Soportados

| Gesto | Comando | DescripciÃ³n |
|-------|---------|-------------|
| âœ‹ Palma adelante | PITCH_FORWARD | Mover dron hacia adelante |
| ğŸ–ï¸ Palma vertical | PITCH_BACKWARD | Mover dron hacia atrÃ¡s |
| âœŒï¸ V-dedos derecha | ROLL_RIGHT | Mover lateralmente a la derecha |
| âœŒï¸ V-dedos izquierda | ROLL_LEFT | Mover lateralmente a la izquierda |
| ğŸ‘ Pulgar arriba | THROTTLE_UP | Subir altitud |
| ğŸ‘ Pulgar abajo | THROTTLE_DOWN | Bajar altitud |
| ğŸ¤™ Shaka derecha | YAW_RIGHT | Rotar en sentido horario |
| ğŸ¤™ Shaka izquierda | YAW_LEFT | Rotar en sentido antihorario |
| âœŠ PuÃ±o cerrado | HOVER | Mantener posiciÃ³n |
| ğŸ–– Vulcano | EMERGENCY_STOP | Parada de emergencia |

---

## ğŸ“ Estructura del Proyecto

```
drone_gesture_control/
â”œâ”€â”€ config.py                 # ConfiguraciÃ³n global
â”œâ”€â”€ main.py                   # Script principal
â”œâ”€â”€ dataset_recorder.py       # Grabador de dataset
â”œâ”€â”€ inference.py              # Sistema de inferencia en tiempo real
â”œâ”€â”€ drone_simulator.py        # Simulador 3D del dron
â”œâ”€â”€ datasets.py               # Clases de Dataset PyTorch
â”œâ”€â”€ training_utils.py         # Utilidades de entrenamiento
â”œâ”€â”€ train_classifier.py       # Entrenamiento de CNN
â”œâ”€â”€ train_segmentation.py     # Entrenamiento de UNet
â”œâ”€â”€ train_temporal.py         # Entrenamiento de GRU
â”œâ”€â”€ train_colab.ipynb         # Notebook para Google Colab
â”œâ”€â”€ requirements.txt          # Dependencias
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ classifier.py         # Modelo clasificador CNN
â”‚   â”œâ”€â”€ segmentation.py       # Modelo UNet
â”‚   â””â”€â”€ temporal.py           # Modelo GRU temporal
â”œâ”€â”€ data/
â”‚   â””â”€â”€ dataset/              # Dataset de gestos
â”œâ”€â”€ checkpoints/              # Modelos entrenados
â”œâ”€â”€ results/                  # Resultados y grÃ¡ficos
â””â”€â”€ logs/                     # Logs de entrenamiento
```

---

## ğŸš€ InstalaciÃ³n

### 1. Clonar el repositorio
```bash
git clone <tu-repositorio>
cd drone_gesture_control
```

### 2. Crear entorno virtual
```bash
python -m venv venv
source venv/bin/activate  # Linux/Mac
# o
venv\Scripts\activate     # Windows
```

### 3. Instalar dependencias
```bash
# Para GPU NVIDIA (CUDA 11.8)
pip install torch torchvision --index-url https://download.pytorch.org/whl/cu118

# Resto de dependencias
pip install -r requirements.txt
```

---

## ğŸ“Š Flujo de Trabajo

### Paso 1: Grabar Dataset

```bash
python main.py --mode record
```

**Controles del grabador:**
- `0-9`: Seleccionar clase de gesto
- `ESPACIO`: Iniciar/Pausar grabaciÃ³n
- `S`: Guardar estadÃ­sticas
- `T`: Guardar secuencia temporal
- `Q`: Salir

**Recomendaciones de grabaciÃ³n:**
- 8-10 minutos por gesto
- Variar iluminaciÃ³n (natural, artificial)
- Variar distancia a la cÃ¡mara (50cm, 1m, 1.5m)
- Variar Ã¡ngulos de la mano
- Total: ~1.5-2 horas de grabaciÃ³n

### Paso 2: Entrenar Modelos

#### OpciÃ³n A: Local (GPU NVIDIA)
```bash
# Entrenar clasificador CNN
python train_classifier.py --epochs 30 --batch_size 32

# Entrenar red de segmentaciÃ³n
python train_segmentation.py --epochs 50

# Entrenar red temporal
python train_temporal.py --epochs 50
```

#### OpciÃ³n B: Google Colab
1. Subir dataset a Google Drive
2. Abrir `train_colab.ipynb` en Colab
3. Ejecutar todas las celdas
4. Descargar checkpoints

### Paso 3: Ejecutar Sistema

```bash
# Solo demo de inferencia
python main.py --mode demo

# Solo simulador (control por teclado)
python main.py --mode simulator

# Sistema integrado completo
python main.py --mode integrated
```

---

## ğŸ¯ Modos de EjecuciÃ³n

### Demo Mode
```bash
python main.py --mode demo
```
Muestra la detecciÃ³n de gestos en tiempo real con la webcam. Ãštil para probar el sistema de inferencia.

### Simulator Mode
```bash
python main.py --mode simulator
```
Ejecuta el simulador 3D del dron con control por teclado:
- `W/S`: Pitch (adelante/atrÃ¡s)
- `A/D`: Roll (izquierda/derecha)
- `Q/E`: Yaw (rotaciÃ³n)
- `ESPACIO/SHIFT`: Throttle (subir/bajar)
- `H`: Hover
- `X`: Emergencia
- `R`: Reset
- `ESC`: Salir

### Integrated Mode
```bash
python main.py --mode integrated
```
Sistema completo: la webcam captura gestos que controlan el dron en el simulador 3D.

---

## ğŸ“ˆ Arquitectura de Redes

### Red Clasificadora (CNN)
- **Backbone**: ResNet18 pre-entrenado en ImageNet
- **Entrada**: ImÃ¡genes 224x224
- **Salida**: 11 clases de gestos
- **Transfer Learning**: Fine-tuning de todas las capas

### Red de SegmentaciÃ³n (UNet)
- **Encoder**: MobileNetV2 pre-entrenado
- **Entrada**: ImÃ¡genes 256x256
- **Salida**: MÃ¡scara binaria (mano/fondo)
- **Loss**: Dice + BCE combinado

### Red Temporal (GRU)
- **Entrada**: Secuencia de 15 frames (CNN features + landmarks)
- **Hidden Size**: 256
- **Layers**: 2
- **Salidas**: 
  - ClasificaciÃ³n de gesto
  - Intensidad del movimiento (0-1)

---

## ğŸ“Š MÃ©tricas Objetivo

| Modelo | MÃ©trica | Objetivo |
|--------|---------|----------|
| Clasificador CNN | Accuracy | >95% |
| SegmentaciÃ³n UNet | IoU | >90% |
| Red Temporal GRU | Accuracy | >90% |
| Sistema completo | Latencia | <100ms |
| Sistema completo | FPS | â‰¥20 |

---

## ğŸ› ï¸ ConfiguraciÃ³n

Editar `config.py` para ajustar:

```python
# ConfiguraciÃ³n de cÃ¡mara
CAMERA_CONFIG = {
    "camera_id": 0,
    "frame_width": 640,
    "frame_height": 480,
    "fps": 30,
}

# ConfiguraciÃ³n de entrenamiento
TRAINING_CONFIG = {
    "device": "cuda",
    "cls_epochs": 30,
    "cls_batch_size": 32,
    "cls_lr": 1e-4,
    # ...
}

# ConfiguraciÃ³n de inferencia
INFERENCE_CONFIG = {
    "confidence_threshold": 0.7,
    "smoothing_window": 5,
    "gesture_hold_frames": 3,
}
```

---

## ğŸ”§ SoluciÃ³n de Problemas

### Error: CUDA not available
```bash
# Verificar instalaciÃ³n de CUDA
nvidia-smi
python -c "import torch; print(torch.cuda.is_available())"

# Reinstalar PyTorch con CUDA
pip install torch torchvision --index-url https://download.pytorch.org/whl/cu118
```

### Error: No camera found
```python
# Cambiar ID de cÃ¡mara en config.py
CAMERA_CONFIG["camera_id"] = 1  # Probar 0, 1, 2...
```

### Error: OpenGL not working
```bash
# Usar modo 2D alternativo
python drone_simulator.py --2d
```

### Baja precisiÃ³n de gestos
- Aumentar cantidad de datos de entrenamiento
- Variar condiciones de grabaciÃ³n
- Ajustar `confidence_threshold` en config.py

---

## ğŸ“š Referencias

- [MediaPipe Hands](https://google.github.io/mediapipe/solutions/hands.html)
- [HaGRID Dataset](https://github.com/hukenovs/hagrid)
- [Segmentation Models PyTorch](https://github.com/qubvel/segmentation_models.pytorch)
- [PyTorch Documentation](https://pytorch.org/docs/)

---

## ğŸ‘¤ Autor

**Pedro** - Proyecto Final de Inteligencia Artificial

---

## ğŸ“„ Licencia

Este proyecto es para uso acadÃ©mico.
