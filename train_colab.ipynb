{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# üöÅ Drone Gesture Control - Entrenamiento en Google Colab\n",
        "\n",
        "## Proyecto Final - Inteligencia Artificial\n",
        "\n",
        "Este notebook entrena los modelos necesarios para el control de dron con gestos:\n",
        "1. **Red de Segmentaci√≥n (UNet)** - Segmenta la mano del fondo\n",
        "2. **Red Clasificadora (CNN)** - Clasifica el gesto realizado\n",
        "3. **Red Temporal (GRU)** - Analiza secuencias para suavizado e intensidad\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "intro"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Configuraci√≥n del Entorno"
      ],
      "metadata": {
        "id": "setup"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install_deps"
      },
      "outputs": [],
      "source": [
        "# Instalar dependencias\n",
        "!pip install -q torch torchvision\n",
        "!pip install -q segmentation-models-pytorch\n",
        "!pip install -q timm\n",
        "!pip install -q mediapipe\n",
        "!pip install -q scikit-learn\n",
        "!pip install -q seaborn\n",
        "!pip install -q tqdm\n",
        "\n",
        "print(\"‚úì Dependencias instaladas\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Verificar GPU\n",
        "import torch\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
      ],
      "metadata": {
        "id": "check_gpu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Montar Google Drive para guardar checkpoints\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Crear directorio del proyecto\n",
        "import os\n",
        "PROJECT_DIR = '/content/drive/MyDrive/drone_gesture_control'\n",
        "os.makedirs(PROJECT_DIR, exist_ok=True)\n",
        "os.makedirs(f'{PROJECT_DIR}/checkpoints', exist_ok=True)\n",
        "os.makedirs(f'{PROJECT_DIR}/results', exist_ok=True)\n",
        "print(f\"‚úì Proyecto en: {PROJECT_DIR}\")"
      ],
      "metadata": {
        "id": "mount_drive"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Subir Dataset\n",
        "\n",
        "Sube tu dataset grabado con `dataset_recorder.py` a Google Drive en la carpeta:\n",
        "`/drone_gesture_control/data/dataset/`\n",
        "\n",
        "Estructura esperada:\n",
        "```\n",
        "dataset/\n",
        "‚îú‚îÄ‚îÄ images/\n",
        "‚îÇ   ‚îú‚îÄ‚îÄ PITCH_FORWARD/\n",
        "‚îÇ   ‚îú‚îÄ‚îÄ PITCH_BACKWARD/\n",
        "‚îÇ   ‚îî‚îÄ‚îÄ ...\n",
        "‚îú‚îÄ‚îÄ masks/\n",
        "‚îÇ   ‚îî‚îÄ‚îÄ ...\n",
        "‚îú‚îÄ‚îÄ landmarks/\n",
        "‚îÇ   ‚îî‚îÄ‚îÄ ...\n",
        "‚îî‚îÄ‚îÄ sequences/\n",
        "    ‚îî‚îÄ‚îÄ ...\n",
        "```"
      ],
      "metadata": {
        "id": "upload_dataset"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Verificar dataset\n",
        "DATASET_DIR = f'{PROJECT_DIR}/data/dataset'\n",
        "\n",
        "if os.path.exists(DATASET_DIR):\n",
        "    print(\"Estructura del dataset:\")\n",
        "    for root, dirs, files in os.walk(DATASET_DIR):\n",
        "        level = root.replace(DATASET_DIR, '').count(os.sep)\n",
        "        indent = ' ' * 2 * level\n",
        "        print(f\"{indent}{os.path.basename(root)}/\")\n",
        "        if level < 2:\n",
        "            subindent = ' ' * 2 * (level + 1)\n",
        "            for file in files[:3]:\n",
        "                print(f\"{subindent}{file}\")\n",
        "            if len(files) > 3:\n",
        "                print(f\"{subindent}... ({len(files)} archivos)\")\n",
        "else:\n",
        "    print(f\"‚ö†Ô∏è Dataset no encontrado en {DATASET_DIR}\")\n",
        "    print(\"Por favor sube tu dataset a Google Drive\")"
      ],
      "metadata": {
        "id": "verify_dataset"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Configuraci√≥n del Proyecto"
      ],
      "metadata": {
        "id": "config"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Configuraci√≥n adaptada para Colab\n",
        "from pathlib import Path\n",
        "\n",
        "# Rutas\n",
        "PROJECT_ROOT = Path(PROJECT_DIR)\n",
        "DATA_DIR = PROJECT_ROOT / \"data\"\n",
        "DATASET_DIR = DATA_DIR / \"dataset\"\n",
        "CHECKPOINTS_DIR = PROJECT_ROOT / \"checkpoints\"\n",
        "RESULTS_DIR = PROJECT_ROOT / \"results\"\n",
        "\n",
        "# Clases de gestos\n",
        "GESTURE_CLASSES = {\n",
        "    0: \"PITCH_FORWARD\",\n",
        "    1: \"PITCH_BACKWARD\",\n",
        "    2: \"ROLL_RIGHT\",\n",
        "    3: \"ROLL_LEFT\",\n",
        "    4: \"THROTTLE_UP\",\n",
        "    5: \"THROTTLE_DOWN\",\n",
        "    6: \"YAW_RIGHT\",\n",
        "    7: \"YAW_LEFT\",\n",
        "    8: \"HOVER\",\n",
        "    9: \"EMERGENCY_STOP\",\n",
        "    10: \"NO_GESTURE\"\n",
        "}\n",
        "NUM_CLASSES = len(GESTURE_CLASSES)\n",
        "\n",
        "# Configuraci√≥n de entrenamiento\n",
        "TRAINING_CONFIG = {\n",
        "    \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
        "    \"seed\": 42,\n",
        "    \n",
        "    # Segmentaci√≥n\n",
        "    \"seg_epochs\": 30,\n",
        "    \"seg_batch_size\": 16,\n",
        "    \"seg_lr\": 1e-4,\n",
        "    \n",
        "    # Clasificador\n",
        "    \"cls_epochs\": 25,\n",
        "    \"cls_batch_size\": 32,\n",
        "    \"cls_lr\": 1e-4,\n",
        "    \n",
        "    # Temporal\n",
        "    \"temp_epochs\": 30,\n",
        "    \"temp_batch_size\": 16,\n",
        "    \"temp_lr\": 1e-3,\n",
        "    \n",
        "    # General\n",
        "    \"train_split\": 0.7,\n",
        "    \"val_split\": 0.15,\n",
        "    \"test_split\": 0.15,\n",
        "    \"patience\": 10,\n",
        "    \"augmentation\": True,\n",
        "}\n",
        "\n",
        "print(f\"Device: {TRAINING_CONFIG['device']}\")\n",
        "print(f\"Clases: {NUM_CLASSES}\")"
      ],
      "metadata": {
        "id": "config_code"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Datasets"
      ],
      "metadata": {
        "id": "datasets"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "class GestureClassificationDataset(Dataset):\n",
        "    \"\"\"Dataset para clasificaci√≥n de gestos.\"\"\"\n",
        "    \n",
        "    def __init__(self, root_dir, transform=None, split='train'):\n",
        "        self.root_dir = Path(root_dir)\n",
        "        self.transform = transform\n",
        "        self.split = split\n",
        "        self.images_dir = self.root_dir / \"images\"\n",
        "        \n",
        "        # Recolectar muestras\n",
        "        self.samples = []\n",
        "        for class_id, class_name in GESTURE_CLASSES.items():\n",
        "            class_dir = self.images_dir / class_name\n",
        "            if not class_dir.exists():\n",
        "                continue\n",
        "            for img_file in class_dir.glob(\"*.jpg\"):\n",
        "                self.samples.append((img_file, class_id))\n",
        "        \n",
        "        # Shuffle y split\n",
        "        random.seed(TRAINING_CONFIG[\"seed\"])\n",
        "        random.shuffle(self.samples)\n",
        "        \n",
        "        n = len(self.samples)\n",
        "        train_end = int(n * TRAINING_CONFIG[\"train_split\"])\n",
        "        val_end = train_end + int(n * TRAINING_CONFIG[\"val_split\"])\n",
        "        \n",
        "        if split == 'train':\n",
        "            self.samples = self.samples[:train_end]\n",
        "        elif split == 'val':\n",
        "            self.samples = self.samples[train_end:val_end]\n",
        "        elif split == 'test':\n",
        "            self.samples = self.samples[val_end:]\n",
        "        \n",
        "        # Transform por defecto\n",
        "        if self.transform is None:\n",
        "            if split == 'train' and TRAINING_CONFIG[\"augmentation\"]:\n",
        "                self.transform = transforms.Compose([\n",
        "                    transforms.Resize((224, 224)),\n",
        "                    transforms.RandomHorizontalFlip(),\n",
        "                    transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3),\n",
        "                    transforms.RandomRotation(20),\n",
        "                    transforms.ToTensor(),\n",
        "                    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "                ])\n",
        "            else:\n",
        "                self.transform = transforms.Compose([\n",
        "                    transforms.Resize((224, 224)),\n",
        "                    transforms.ToTensor(),\n",
        "                    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "                ])\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        img_path, class_id = self.samples[idx]\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "        image = self.transform(image)\n",
        "        return image, class_id\n",
        "\n",
        "\n",
        "class SegmentationDataset(Dataset):\n",
        "    \"\"\"Dataset para segmentaci√≥n.\"\"\"\n",
        "    \n",
        "    def __init__(self, root_dir, transform=None, mask_transform=None, split='train'):\n",
        "        self.root_dir = Path(root_dir)\n",
        "        self.split = split\n",
        "        self.images_dir = self.root_dir / \"images\"\n",
        "        self.masks_dir = self.root_dir / \"masks\"\n",
        "        \n",
        "        # Recolectar muestras\n",
        "        self.samples = []\n",
        "        for class_name in GESTURE_CLASSES.values():\n",
        "            class_img_dir = self.images_dir / class_name\n",
        "            class_mask_dir = self.masks_dir / class_name\n",
        "            if not class_img_dir.exists():\n",
        "                continue\n",
        "            for img_file in class_img_dir.glob(\"*.jpg\"):\n",
        "                mask_file = class_mask_dir / f\"{img_file.stem}.png\"\n",
        "                if mask_file.exists():\n",
        "                    self.samples.append((img_file, mask_file))\n",
        "        \n",
        "        # Shuffle y split\n",
        "        random.seed(TRAINING_CONFIG[\"seed\"])\n",
        "        random.shuffle(self.samples)\n",
        "        \n",
        "        n = len(self.samples)\n",
        "        train_end = int(n * TRAINING_CONFIG[\"train_split\"])\n",
        "        val_end = train_end + int(n * TRAINING_CONFIG[\"val_split\"])\n",
        "        \n",
        "        if split == 'train':\n",
        "            self.samples = self.samples[:train_end]\n",
        "        elif split == 'val':\n",
        "            self.samples = self.samples[train_end:val_end]\n",
        "        elif split == 'test':\n",
        "            self.samples = self.samples[val_end:]\n",
        "        \n",
        "        # Transforms\n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.Resize((256, 256)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "        ])\n",
        "        self.mask_transform = transforms.Compose([\n",
        "            transforms.Resize((256, 256)),\n",
        "            transforms.ToTensor()\n",
        "        ])\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        img_path, mask_path = self.samples[idx]\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "        mask = Image.open(mask_path).convert('L')\n",
        "        \n",
        "        image = self.transform(image)\n",
        "        mask = self.mask_transform(mask)\n",
        "        mask = (mask > 0.5).float()\n",
        "        \n",
        "        return image, mask\n",
        "\n",
        "\n",
        "# Crear dataloaders\n",
        "def get_dataloaders(dataset_type='classification', batch_size=32):\n",
        "    if dataset_type == 'classification':\n",
        "        train_ds = GestureClassificationDataset(DATASET_DIR, split='train')\n",
        "        val_ds = GestureClassificationDataset(DATASET_DIR, split='val')\n",
        "        test_ds = GestureClassificationDataset(DATASET_DIR, split='test')\n",
        "    else:\n",
        "        train_ds = SegmentationDataset(DATASET_DIR, split='train')\n",
        "        val_ds = SegmentationDataset(DATASET_DIR, split='val')\n",
        "        test_ds = SegmentationDataset(DATASET_DIR, split='test')\n",
        "    \n",
        "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "    val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "    test_loader = DataLoader(test_ds, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "    \n",
        "    print(f\"Dataset {dataset_type}: Train={len(train_ds)}, Val={len(val_ds)}, Test={len(test_ds)}\")\n",
        "    \n",
        "    return train_loader, val_loader, test_loader\n",
        "\n",
        "# Verificar datasets\n",
        "if DATASET_DIR.exists():\n",
        "    train_loader, val_loader, test_loader = get_dataloaders('classification')\n",
        "    for images, labels in train_loader:\n",
        "        print(f\"Batch shape: {images.shape}, Labels: {labels[:5]}\")\n",
        "        break"
      ],
      "metadata": {
        "id": "datasets_code"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Modelos"
      ],
      "metadata": {
        "id": "models"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import models\n",
        "\n",
        "# Clasificador CNN\n",
        "class GestureClassifier(nn.Module):\n",
        "    def __init__(self, num_classes=NUM_CLASSES, pretrained=True):\n",
        "        super().__init__()\n",
        "        self.backbone = models.resnet18(weights='IMAGENET1K_V1' if pretrained else None)\n",
        "        self.feature_dim = self.backbone.fc.in_features\n",
        "        self.backbone.fc = nn.Identity()\n",
        "        \n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(self.feature_dim, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(256, num_classes)\n",
        "        )\n",
        "    \n",
        "    def forward(self, x):\n",
        "        features = self.backbone(x)\n",
        "        return self.classifier(features)\n",
        "\n",
        "\n",
        "# UNet para segmentaci√≥n\n",
        "try:\n",
        "    import segmentation_models_pytorch as smp\n",
        "    \n",
        "    class SegmentationModel(nn.Module):\n",
        "        def __init__(self):\n",
        "            super().__init__()\n",
        "            self.model = smp.Unet(\n",
        "                encoder_name=\"mobilenet_v2\",\n",
        "                encoder_weights=\"imagenet\",\n",
        "                in_channels=3,\n",
        "                classes=2\n",
        "            )\n",
        "        \n",
        "        def forward(self, x):\n",
        "            return self.model(x)\n",
        "except ImportError:\n",
        "    print(\"SMP no disponible, usando UNet b√°sico\")\n",
        "\n",
        "\n",
        "# Red Temporal GRU\n",
        "class TemporalGRU(nn.Module):\n",
        "    def __init__(self, input_size=512+63, hidden_size=256, num_layers=2, num_classes=NUM_CLASSES):\n",
        "        super().__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        \n",
        "        self.input_proj = nn.Linear(input_size, hidden_size)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size, num_layers, \n",
        "                          batch_first=True, dropout=0.3)\n",
        "        \n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(hidden_size, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(hidden_size, num_classes)\n",
        "        )\n",
        "        \n",
        "        self.intensity = nn.Sequential(\n",
        "            nn.Linear(hidden_size, hidden_size // 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size // 2, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = self.input_proj(x)\n",
        "        output, hidden = self.gru(x)\n",
        "        context = hidden[-1]\n",
        "        \n",
        "        logits = self.classifier(context)\n",
        "        intensity = self.intensity(context).squeeze(-1)\n",
        "        \n",
        "        return logits, intensity\n",
        "\n",
        "\n",
        "# Modelo completo CNN + GRU\n",
        "class GestureSequenceModel(nn.Module):\n",
        "    def __init__(self, pretrained=True, freeze_cnn=True):\n",
        "        super().__init__()\n",
        "        \n",
        "        # CNN backbone\n",
        "        self.cnn = models.mobilenet_v3_small(weights='IMAGENET1K_V1' if pretrained else None)\n",
        "        self.cnn_dim = self.cnn.classifier[0].in_features\n",
        "        self.cnn.classifier = nn.Identity()\n",
        "        \n",
        "        if freeze_cnn:\n",
        "            for p in self.cnn.parameters():\n",
        "                p.requires_grad = False\n",
        "        \n",
        "        # Landmark projection\n",
        "        self.landmark_proj = nn.Linear(63, 64)\n",
        "        \n",
        "        # Temporal\n",
        "        self.temporal = TemporalGRU(input_size=self.cnn_dim + 64)\n",
        "    \n",
        "    def forward(self, frames, landmarks=None):\n",
        "        B, T, C, H, W = frames.shape\n",
        "        \n",
        "        frames_flat = frames.view(B * T, C, H, W)\n",
        "        cnn_features = self.cnn(frames_flat)\n",
        "        cnn_features = cnn_features.view(B, T, -1)\n",
        "        \n",
        "        if landmarks is not None:\n",
        "            lm_features = self.landmark_proj(landmarks)\n",
        "            combined = torch.cat([cnn_features, lm_features], dim=2)\n",
        "        else:\n",
        "            zeros = torch.zeros(B, T, 64, device=frames.device)\n",
        "            combined = torch.cat([cnn_features, zeros], dim=2)\n",
        "        \n",
        "        return self.temporal(combined)\n",
        "\n",
        "\n",
        "# Test modelos\n",
        "print(\"\\nTest de modelos:\")\n",
        "\n",
        "model = GestureClassifier()\n",
        "x = torch.randn(2, 3, 224, 224)\n",
        "y = model(x)\n",
        "print(f\"Clasificador: {x.shape} -> {y.shape}\")\n",
        "\n",
        "model = TemporalGRU()\n",
        "x = torch.randn(2, 15, 575)\n",
        "logits, intensity = model(x)\n",
        "print(f\"GRU: {x.shape} -> logits={logits.shape}, intensity={intensity.shape}\")"
      ],
      "metadata": {
        "id": "models_code"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Funciones de Entrenamiento"
      ],
      "metadata": {
        "id": "training_functions"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import seaborn as sns\n",
        "\n",
        "def train_epoch(model, dataloader, criterion, optimizer, device):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    running_correct = 0\n",
        "    running_total = 0\n",
        "    \n",
        "    for images, labels in tqdm(dataloader, desc=\"Training\"):\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        running_loss += loss.item() * images.size(0)\n",
        "        running_correct += (predicted == labels).sum().item()\n",
        "        running_total += labels.size(0)\n",
        "    \n",
        "    return running_loss / running_total, running_correct / running_total\n",
        "\n",
        "\n",
        "def validate(model, dataloader, criterion, device, return_preds=False):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    running_correct = 0\n",
        "    running_total = 0\n",
        "    all_preds, all_labels = [], []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for images, labels in tqdm(dataloader, desc=\"Validating\"):\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "            \n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            \n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            running_loss += loss.item() * images.size(0)\n",
        "            running_correct += (predicted == labels).sum().item()\n",
        "            running_total += labels.size(0)\n",
        "            \n",
        "            if return_preds:\n",
        "                all_preds.extend(predicted.cpu().numpy())\n",
        "                all_labels.extend(labels.cpu().numpy())\n",
        "    \n",
        "    if return_preds:\n",
        "        return running_loss / running_total, running_correct / running_total, all_preds, all_labels\n",
        "    return running_loss / running_total, running_correct / running_total\n",
        "\n",
        "\n",
        "def plot_training_curves(train_losses, train_accs, val_losses, val_accs, title=\"Training Curves\"):\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "    epochs = range(1, len(train_losses) + 1)\n",
        "    \n",
        "    axes[0].plot(epochs, train_losses, 'b-', label='Train')\n",
        "    axes[0].plot(epochs, val_losses, 'r-', label='Val')\n",
        "    axes[0].set_xlabel('Epoch')\n",
        "    axes[0].set_ylabel('Loss')\n",
        "    axes[0].legend()\n",
        "    axes[0].set_title('Loss')\n",
        "    axes[0].grid(True)\n",
        "    \n",
        "    axes[1].plot(epochs, train_accs, 'b-', label='Train')\n",
        "    axes[1].plot(epochs, val_accs, 'r-', label='Val')\n",
        "    axes[1].set_xlabel('Epoch')\n",
        "    axes[1].set_ylabel('Accuracy')\n",
        "    axes[1].legend()\n",
        "    axes[1].set_title('Accuracy')\n",
        "    axes[1].grid(True)\n",
        "    \n",
        "    fig.suptitle(title, fontsize=14, fontweight='bold')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(RESULTS_DIR / f\"{title.replace(' ', '_').lower()}.png\", dpi=150)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_confusion_matrix(preds, labels, class_names):\n",
        "    cm = confusion_matrix(labels, preds)\n",
        "    plt.figure(figsize=(12, 10))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=class_names, yticklabels=class_names)\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('True')\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.xticks(rotation=45, ha='right')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(RESULTS_DIR / \"confusion_matrix.png\", dpi=150)\n",
        "    plt.show()\n",
        "\n",
        "print(\"‚úì Funciones de entrenamiento definidas\")"
      ],
      "metadata": {
        "id": "training_functions_code"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Entrenar Clasificador CNN"
      ],
      "metadata": {
        "id": "train_classifier"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Configuraci√≥n\n",
        "EPOCHS = TRAINING_CONFIG[\"cls_epochs\"]\n",
        "BATCH_SIZE = TRAINING_CONFIG[\"cls_batch_size\"]\n",
        "LR = TRAINING_CONFIG[\"cls_lr\"]\n",
        "DEVICE = TRAINING_CONFIG[\"device\"]\n",
        "\n",
        "print(f\"\\n{'='*50}\")\n",
        "print(\"ENTRENAMIENTO DE CLASIFICADOR CNN\")\n",
        "print(f\"{'='*50}\")\n",
        "print(f\"Epochs: {EPOCHS}\")\n",
        "print(f\"Batch size: {BATCH_SIZE}\")\n",
        "print(f\"Learning rate: {LR}\")\n",
        "print(f\"Device: {DEVICE}\")\n",
        "\n",
        "# Datasets\n",
        "train_loader, val_loader, test_loader = get_dataloaders('classification', BATCH_SIZE)\n",
        "\n",
        "# Modelo\n",
        "model = GestureClassifier(pretrained=True).to(DEVICE)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=1e-5)\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=10)\n",
        "\n",
        "# Tracking\n",
        "train_losses, train_accs = [], []\n",
        "val_losses, val_accs = [], []\n",
        "best_val_acc = 0.0\n",
        "\n",
        "# Entrenamiento\n",
        "for epoch in range(EPOCHS):\n",
        "    print(f\"\\nEpoch {epoch+1}/{EPOCHS}\")\n",
        "    \n",
        "    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, DEVICE)\n",
        "    val_loss, val_acc = validate(model, val_loader, criterion, DEVICE)\n",
        "    \n",
        "    train_losses.append(train_loss)\n",
        "    train_accs.append(train_acc)\n",
        "    val_losses.append(val_loss)\n",
        "    val_accs.append(val_acc)\n",
        "    \n",
        "    print(f\"  Train - Loss: {train_loss:.4f}, Acc: {train_acc:.4f}\")\n",
        "    print(f\"  Val   - Loss: {val_loss:.4f}, Acc: {val_acc:.4f}\")\n",
        "    \n",
        "    scheduler.step()\n",
        "    \n",
        "    # Guardar mejor modelo\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'val_acc': val_acc,\n",
        "        }, CHECKPOINTS_DIR / 'classifier_resnet18_best.pt')\n",
        "        print(f\"  ‚úì Mejor modelo guardado (acc={val_acc:.4f})\")\n",
        "\n",
        "print(f\"\\n¬°Entrenamiento completado! Mejor accuracy: {best_val_acc:.4f}\")"
      ],
      "metadata": {
        "id": "train_classifier_code"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizar curvas de entrenamiento\n",
        "plot_training_curves(train_losses, train_accs, val_losses, val_accs, \n",
        "                     title=\"Clasificador CNN\")"
      ],
      "metadata": {
        "id": "plot_classifier"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluaci√≥n en test\n",
        "checkpoint = torch.load(CHECKPOINTS_DIR / 'classifier_resnet18_best.pt')\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "test_loss, test_acc, preds, labels = validate(model, test_loader, criterion, DEVICE, return_preds=True)\n",
        "print(f\"\\nTest - Loss: {test_loss:.4f}, Accuracy: {test_acc:.4f}\")\n",
        "\n",
        "# Matriz de confusi√≥n\n",
        "class_names = [GESTURE_CLASSES[i][:10] for i in range(NUM_CLASSES)]\n",
        "plot_confusion_matrix(preds, labels, class_names)\n",
        "\n",
        "# Reporte\n",
        "print(\"\\nReporte de clasificaci√≥n:\")\n",
        "print(classification_report(labels, preds, target_names=class_names, zero_division=0))"
      ],
      "metadata": {
        "id": "eval_classifier"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8. Entrenar Red de Segmentaci√≥n"
      ],
      "metadata": {
        "id": "train_segmentation"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dice + BCE Loss\n",
        "class DiceBCELoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.bce = nn.BCEWithLogitsLoss()\n",
        "    \n",
        "    def forward(self, logits, targets):\n",
        "        bce_loss = self.bce(logits[:, 1, :, :], targets.squeeze(1))\n",
        "        probs = torch.sigmoid(logits[:, 1, :, :])\n",
        "        intersection = (probs * targets.squeeze(1)).sum(dim=(1, 2))\n",
        "        union = probs.sum(dim=(1, 2)) + targets.squeeze(1).sum(dim=(1, 2))\n",
        "        dice_loss = 1 - (2. * intersection + 1e-6) / (union + 1e-6)\n",
        "        return 0.5 * bce_loss + 0.5 * dice_loss.mean()\n",
        "\n",
        "\n",
        "def calculate_iou(pred, target):\n",
        "    pred = pred.view(-1)\n",
        "    target = target.view(-1)\n",
        "    intersection = (pred * target).sum()\n",
        "    union = pred.sum() + target.sum() - intersection\n",
        "    return ((intersection + 1e-6) / (union + 1e-6)).item()\n",
        "\n",
        "\n",
        "# Configuraci√≥n\n",
        "EPOCHS = TRAINING_CONFIG[\"seg_epochs\"]\n",
        "BATCH_SIZE = TRAINING_CONFIG[\"seg_batch_size\"]\n",
        "LR = TRAINING_CONFIG[\"seg_lr\"]\n",
        "\n",
        "print(f\"\\n{'='*50}\")\n",
        "print(\"ENTRENAMIENTO DE RED DE SEGMENTACI√ìN\")\n",
        "print(f\"{'='*50}\")\n",
        "\n",
        "# Datasets\n",
        "train_loader, val_loader, test_loader = get_dataloaders('segmentation', BATCH_SIZE)\n",
        "\n",
        "# Modelo\n",
        "seg_model = SegmentationModel().to(DEVICE)\n",
        "criterion = DiceBCELoss()\n",
        "optimizer = torch.optim.AdamW(seg_model.parameters(), lr=LR)\n",
        "\n",
        "# Tracking\n",
        "train_losses, train_ious = [], []\n",
        "val_losses, val_ious = [], []\n",
        "best_iou = 0.0\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    print(f\"\\nEpoch {epoch+1}/{EPOCHS}\")\n",
        "    \n",
        "    # Train\n",
        "    seg_model.train()\n",
        "    epoch_loss, epoch_iou = 0.0, 0.0\n",
        "    for images, masks in tqdm(train_loader, desc=\"Training\"):\n",
        "        images = images.to(DEVICE)\n",
        "        masks = masks.to(DEVICE)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        outputs = seg_model(images)\n",
        "        loss = criterion(outputs, masks)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            pred = (torch.sigmoid(outputs[:, 1]) > 0.5).float()\n",
        "            iou = calculate_iou(pred, masks.squeeze(1))\n",
        "        \n",
        "        epoch_loss += loss.item()\n",
        "        epoch_iou += iou\n",
        "    \n",
        "    train_losses.append(epoch_loss / len(train_loader))\n",
        "    train_ious.append(epoch_iou / len(train_loader))\n",
        "    \n",
        "    # Validate\n",
        "    seg_model.eval()\n",
        "    val_loss, val_iou = 0.0, 0.0\n",
        "    with torch.no_grad():\n",
        "        for images, masks in val_loader:\n",
        "            images = images.to(DEVICE)\n",
        "            masks = masks.to(DEVICE)\n",
        "            outputs = seg_model(images)\n",
        "            loss = criterion(outputs, masks)\n",
        "            pred = (torch.sigmoid(outputs[:, 1]) > 0.5).float()\n",
        "            val_loss += loss.item()\n",
        "            val_iou += calculate_iou(pred, masks.squeeze(1))\n",
        "    \n",
        "    val_losses.append(val_loss / len(val_loader))\n",
        "    val_ious.append(val_iou / len(val_loader))\n",
        "    \n",
        "    print(f\"  Train - Loss: {train_losses[-1]:.4f}, IoU: {train_ious[-1]:.4f}\")\n",
        "    print(f\"  Val   - Loss: {val_losses[-1]:.4f}, IoU: {val_ious[-1]:.4f}\")\n",
        "    \n",
        "    if val_ious[-1] > best_iou:\n",
        "        best_iou = val_ious[-1]\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': seg_model.state_dict(),\n",
        "        }, CHECKPOINTS_DIR / 'segmentation_unet_best.pt')\n",
        "        print(f\"  ‚úì Mejor modelo guardado (IoU={best_iou:.4f})\")\n",
        "\n",
        "print(f\"\\n¬°Entrenamiento completado! Mejor IoU: {best_iou:.4f}\")"
      ],
      "metadata": {
        "id": "train_segmentation_code"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizar resultados de segmentaci√≥n\n",
        "plot_training_curves(train_losses, train_ious, val_losses, val_ious,\n",
        "                     title=\"Segmentaci√≥n UNet\")"
      ],
      "metadata": {
        "id": "plot_segmentation"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9. Descargar Modelos"
      ],
      "metadata": {
        "id": "download"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Crear zip con checkpoints\n",
        "import shutil\n",
        "\n",
        "print(\"Archivos en checkpoints:\")\n",
        "for f in CHECKPOINTS_DIR.glob(\"*\"):\n",
        "    size = f.stat().st_size / 1e6\n",
        "    print(f\"  {f.name}: {size:.1f} MB\")\n",
        "\n",
        "# Crear zip\n",
        "shutil.make_archive('/content/checkpoints', 'zip', CHECKPOINTS_DIR)\n",
        "print(f\"\\n‚úì Archivo zip creado: /content/checkpoints.zip\")\n",
        "\n",
        "# Descargar\n",
        "from google.colab import files\n",
        "files.download('/content/checkpoints.zip')"
      ],
      "metadata": {
        "id": "download_code"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 10. Resumen Final"
      ],
      "metadata": {
        "id": "summary"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"RESUMEN DE ENTRENAMIENTO\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Verificar modelos guardados\n",
        "models_info = [\n",
        "    (\"Clasificador CNN\", \"classifier_resnet18_best.pt\"),\n",
        "    (\"Segmentaci√≥n UNet\", \"segmentation_unet_best.pt\"),\n",
        "    (\"Red Temporal GRU\", \"temporal_gru_best.pt\"),\n",
        "]\n",
        "\n",
        "for name, filename in models_info:\n",
        "    path = CHECKPOINTS_DIR / filename\n",
        "    if path.exists():\n",
        "        size = path.stat().st_size / 1e6\n",
        "        print(f\"‚úì {name}: {size:.1f} MB\")\n",
        "    else:\n",
        "        print(f\"‚úó {name}: No entrenado\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"PR√ìXIMOS PASOS\")\n",
        "print(\"=\"*60)\n",
        "print(\"1. Descarga los checkpoints\")\n",
        "print(\"2. Copia a la carpeta 'checkpoints/' del proyecto\")\n",
        "print(\"3. Ejecuta: python main.py --mode integrated\")\n",
        "print(\"=\"*60)"
      ],
      "metadata": {
        "id": "summary_code"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
